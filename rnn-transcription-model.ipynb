{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9234990,"sourceType":"datasetVersion","datasetId":5585954},{"sourceId":9235025,"sourceType":"datasetVersion","datasetId":5585979}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Relevant Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport soundfile as sf\nimport librosa\nimport os\nimport io\nimport re\n\n\nfrom pathlib import Path\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, TimeDistributed, Activation, Input\nfrom keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-02T18:59:40.243876Z","iopub.execute_input":"2024-09-02T18:59:40.244529Z","iopub.status.idle":"2024-09-02T18:59:51.442987Z","shell.execute_reply.started":"2024-09-02T18:59:40.244458Z","shell.execute_reply":"2024-09-02T18:59:51.441526Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Storing the filepath of the testing and training data folders**","metadata":{}},{"cell_type":"code","source":"train1_dir = Path('/kaggle/input/transcription/LibriSpeech/dev-clean')\ntest1_dir = Path('/kaggle/input/test-data-1/LibriSpeech/test-clean')","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:59:51.445575Z","iopub.execute_input":"2024-09-02T18:59:51.446994Z","iopub.status.idle":"2024-09-02T18:59:51.452705Z","shell.execute_reply.started":"2024-09-02T18:59:51.446931Z","shell.execute_reply":"2024-09-02T18:59:51.451252Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#   **Preprocessing the Training Data**","metadata":{}},{"cell_type":"markdown","source":"# **Helper Functions for Audio**","metadata":{}},{"cell_type":"code","source":"def load_flac(path):\n    data, sample_rate = sf.read(path)\n    return data, sample_rate\n\ntarget_sr = 16000 #We want our audio samples to have a sample rate of 16kHz\n\ndef sample_audio(data, prev_sr, target_sr = target_sr): #We want all the sample rates to be consistent across training data\n    resample = librosa.resample(data, orig_sr = prev_sr, target_sr = target_sr)\n    if len(data.shape) == 2:\n        data = np.mean(data, axis=1)\n    return resample\n\ndef normalize_volume(data): #We want the volume of the sample audio to be consistent across all training data\n    return librosa.util.normalize(data)\n\nn_mfcc = 13 #We use the first 13 mfcc coefficients for RNN speech processing models\n\ndef get_mfcc_features(data, sample_rate = target_sr, n_mfcc = n_mfcc):\n    mfccs = librosa.feature.mfcc(y=data, sr = sample_rate, n_mfcc = n_mfcc)\n    return mfccs.T","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:59:51.454368Z","iopub.execute_input":"2024-09-02T18:59:51.454810Z","iopub.status.idle":"2024-09-02T18:59:51.466967Z","shell.execute_reply.started":"2024-09-02T18:59:51.454768Z","shell.execute_reply":"2024-09-02T18:59:51.465690Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#  **Helper functions for Text**","metadata":{}},{"cell_type":"code","source":"#Generates the vocabulary text to fit the tokenizer to\ndef tokenizer_generator(path):\n    for layer1 in path.iterdir():\n        for layer2 in layer1.iterdir():\n            for index in layer2.iterdir():\n                if index.suffix == \".txt\":\n                    with open(index, 'r') as file:\n                        for line in file:\n                            yield line.strip()\n\n\n#Extracts text from files\ndef read_txt(path): \n    with open(path, 'r') as file:\n        lines = file.readlines()\n    \n    text_data = []\n    for line in lines:\n        text = line.split(' ', 1)[1].strip()\n        text_data.append(text)\n    return text_data\n\n#Pads all text to equal lengths\ndef pad_sequence(token_data): \n    padded_data = pad_sequences(token_data, padding='post')\n    return padded_data\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:59:51.468430Z","iopub.execute_input":"2024-09-02T18:59:51.468914Z","iopub.status.idle":"2024-09-02T18:59:51.483047Z","shell.execute_reply.started":"2024-09-02T18:59:51.468859Z","shell.execute_reply":"2024-09-02T18:59:51.481800Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Audio and Text**","metadata":{}},{"cell_type":"code","source":"def load_data(path):   \n    audio_data = []\n    mfcc_array = []\n    text_array = []\n    number = []\n    #Tokenizing the Text Data\n    tokenizer = Tokenizer(char_level = True)\n    tokenizer.fit_on_texts(tokenizer_generator(path))\n                    \n    \n    for flayer_1 in path.iterdir():\n        for flayer_2 in flayer_1.iterdir():\n            num_items = len(list(flayer_2.iterdir())) - 1\n            #print(flayer_2)\n            #numbers = re.findall(r'\\d+', str(flayer_2))\n            layer_2 = str(flayer_2)\n            parts = os.path.normpath(layer_2).split(os.sep)\n            #print(numbers)\n            path_extension = str(parts[-2]) + '-' + str(parts[-1]) + '.trans.txt'\n            txt_path = flayer_2 / path_extension\n            if txt_path.suffix == \".txt\":\n                path = str(txt_path)\n                text_data = read_txt(path)\n                for sentence in text_data:\n                    tokenized_data = [tokenizer.texts_to_sequences(sentence)]\n                    #print(text_data)\n                    text_array.append(tokenized_data)\n            \n            for index in range(num_items):\n                audio_extension = str(parts[-2]) + '-' + str(parts[-1])\n                if index < 10:\n                    audio_extension = audio_extension + '-000' + str(index) + '.flac' \n                elif index < 100:\n                    audio_extension = audio_extension + '-00' + str(index) + '.flac'\n                elif index < 1000:\n                    audio_extension = audio_extension + '-0' + str(index) + '.flac'\n                else:\n                    audio_extension = audio_extension + '-' + str(index) + '.flac'\n                    \n                audio_path = flayer_2 / audio_extension\n                if audio_path.suffix == \".flac\":\n                    data, samplerate = load_flac(audio_path)\n                    resampled_data = sample_audio(data, samplerate)\n                    normalized_data = normalize_volume(resampled_data)\n                    mfcc_features = get_mfcc_features(normalized_data)\n                    mfcc_array.append(mfcc_features)\n    \n    \n    sequence_lengths = [mfcc.shape[0] for mfcc in mfcc_array]  # Assuming shape is (time_steps, n_mfcc)\n    max_length = max(sequence_lengths)\n    \n    for index in range(len(mfcc_array)):\n        pad_width = max_length - mfcc_array[index].shape[0]\n        mfcc_array[index] = np.pad(mfcc_array[index], pad_width=((0, pad_width), (0, 0)), mode='constant')\n    \n    padded_text = []\n    for text in text_array:\n        if len(text) > max_length:\n            padded_text.append(text[:max_length].T)\n        else:\n            padded_text.append(pad_sequences(text, maxlen = max_length, padding = 'post').T)\n    \n    padded_text = np.array(padded_text)\n    padded_text = np.squeeze(padded_text, axis = 1)\n    \n    return mfcc_array, padded_text, max_length, tokenizer\n  \n                           \n\n# Call the function\ninput_data, output_data, audio_max_len, tokenizer = load_data(train1_dir)\n\ninput_data = np.array(input_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:59:51.487005Z","iopub.execute_input":"2024-09-02T18:59:51.487699Z","iopub.status.idle":"2024-09-02T19:02:14.840903Z","shell.execute_reply.started":"2024-09-02T18:59:51.487644Z","shell.execute_reply":"2024-09-02T19:02:14.839768Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(\"Input data shape:\", input_data.shape)\nprint(\"Output data shape:\", output_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:02:14.842440Z","iopub.execute_input":"2024-09-02T19:02:14.843194Z","iopub.status.idle":"2024-09-02T19:02:14.849600Z","shell.execute_reply.started":"2024-09-02T19:02:14.843148Z","shell.execute_reply":"2024-09-02T19:02:14.848359Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Input data shape: (2703, 1021, 13)\nOutput data shape: (2703, 1021, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Model Construction and Training**","metadata":{}},{"cell_type":"code","source":"transcription_model = Sequential([\n    GRU(64, input_shape = (audio_max_len, 13), return_sequences=True),\n    GRU(64, return_sequences=True),\n    TimeDistributed(Dense(units = len(tokenizer.word_index)+1)),\n    #TimeDistributed(Dense(1, activation = 'linear'))\n])\n\ntranscription_model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy', 'mean_squared_error'])\ntranscription_model.summary()\n\ntranscription_model.fit(input_data, output_data, epochs=3, batch_size = 128, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:02:14.851498Z","iopub.execute_input":"2024-09-02T19:02:14.851945Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1021\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m19,968\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1021\u001b[0m, \u001b[38;5;34m40\u001b[0m)       │         \u001b[38;5;34m2,600\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1021</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,968</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1021</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,600</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,568\u001b[0m (88.16 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,568</span> (88.16 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,568\u001b[0m (88.16 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,568</span> (88.16 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Process Test Data**","metadata":{}},{"cell_type":"code","source":"def load_test_data(path, tokenizer, max_audio_size):   \n    audio_data = []\n    mfcc_array = []\n    text_array = []\n    number = []                  \n    \n    for flayer_1 in path.iterdir():\n        for flayer_2 in flayer_1.iterdir():\n            num_items = len(list(flayer_2.iterdir())) - 1\n            layer_2 = str(flayer_2)\n            parts = os.path.normpath(layer_2).split(os.sep)\n            path_extension = str(parts[-2]) + '-' + str(parts[-1]) + '.trans.txt'\n            txt_path = flayer_2 / path_extension\n            if txt_path.suffix == \".txt\":\n                path = str(txt_path)\n                text_data = read_txt(path)\n                for sentence in text_data:\n                    tokenized_data = [tokenizer.texts_to_sequences(sentence)]\n                    token_data = np.array(tokenized_data)\n                    text_array.append(tokenized_data)\n            \n            for index in range(num_items):\n                audio_extension = str(parts[-2]) + '-' + str(parts[-1])\n                if index < 10:\n                    audio_extension = audio_extension + '-000' + str(index) + '.flac' \n                elif index < 100:\n                    audio_extension = audio_extension + '-00' + str(index) + '.flac'\n                elif index < 1000:\n                    audio_extension = audio_extension + '-0' + str(index) + '.flac'\n                else:\n                    audio_extension = audio_extension + '-' + str(index) + '.flac'\n                    \n                audio_path = flayer_2 / audio_extension\n                if audio_path.suffix == \".flac\":\n                    data, samplerate = load_flac(audio_path)\n                    resampled_data = sample_audio(data, samplerate)\n                    normalized_data = normalize_volume(resampled_data)\n                    mfcc_features = get_mfcc_features(normalized_data)\n                    mfcc_array.append(mfcc_features)\n    \n    \n    max_length = max_audio_size\n    \n    for index in range(len(mfcc_array)):\n        if len(mfcc_array[index]) > max_length:\n            mfcc_array[index] = mfcc_array[index][:max_length]\n        else:\n            pad_width = max_length - mfcc_array[index].shape[0]\n            mfcc_array[index] = np.pad(mfcc_array[index], pad_width=((0, pad_width), (0, 0)), mode='constant')\n            \n    \n    padded_text = []\n    for text in text_array:\n       if len(text) > max_length:\n           padded_text.append(text[:max_length].T)\n       else:\n           padded_text.append(pad_sequences(text, maxlen = max_length, padding = 'post').T) \n    \n    \n    padded_text = np.array(padded_text)\n    padded_text = np.squeeze(padded_text, axis = 1)\n    print(padded_text.shape)\n    return mfcc_array, padded_text, max_length, tokenizer\n  \n                           \n\n# Call the function\n\ntest_input, test_output, testmaxlen, test_tokenizer = load_test_data(test1_dir, tokenizer, audio_max_len)\ntest_input = np.array(test_input)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Input data shape:\", input_data.shape)\nprint(\"Output data shape:\", output_data.shape)\nprint(\"Input test data shape:\", test_input.shape)\nprint(\"Output test data shape:\", test_output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = transcription_model.evaluate(test_input, test_output, batch_size=64)\nprint(\"test loss, test acc:\", results)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = np.array(results)\nprint(results.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transcription_model.save('transcription.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(output_data)\ndef unprocess(test, tokenizer):\n    char_dict = tokenizer.index_word  # Mapping from index to character\n    \n    reversed_sequences = []\n    \n    for batch in test:  # Iterate over each batch\n        batch_sequences = []\n        for token_seq in batch:  # Iterate over each sequence in the batch\n            if isinstance(token_seq, np.ndarray):\n                token_seq = token_seq.tolist()  # Convert NumPy array to list\n            \n            # Convert each token in the sequence to the corresponding character\n            chars = ''.join([char_dict[token] for token in token_seq if token in char_dict])\n            batch_sequences.append(chars)\n        \n        reversed_sequences.append(batch_sequences)\n    \n    return reversed_sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"next_input_data = np.expand_dims(test_input[56], axis=0)\ntext_tokens = transcription_model.predict(next_input_data)\nprint(text_tokens.shape)\nprint(text_tokens)\ntext = unprocess(text_tokens, tokenizer)\n#print(text[0])\n\nstring_text = ''.join(text[0])\nprint(string_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_2 = unprocess()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}